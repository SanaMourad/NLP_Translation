{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, LSTM\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "!pip install transformers\n",
        "from transformers import TFAutoModel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ekt4VWLMlFc",
        "outputId": "9067e41f-d454-46a9-a7d0-b1b2ece7fc97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70Tr7ohoHnKR"
      },
      "outputs": [],
      "source": [
        "path_to_data = '/content/ara_eng.txt'\n",
        "translation_file = open(path_to_data, \"r\", encoding='utf-8')\n",
        "raw_data = translation_file.read()\n",
        "translation_file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = raw_data.split('\\n')\n",
        "pairs = [sentence.split('\\t') for sentence in raw_data]\n",
        "pairs = pairs[1000:11000]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z8CL0Zg7H6Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def clean_sentence(sentence):\n",
        "    lower_case_sent = sentence.lower()\n",
        "    string_punctuation = string.punctuation + \"¡\" + '¿'\n",
        "\n",
        "    for punctuation in string_punctuation:\n",
        "        lower_case_sent = lower_case_sent.replace(punctuation, '')\n",
        "\n",
        "    return lower_case_sent\n"
      ],
      "metadata": {
        "id": "GrItf0JzPJNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentences):\n",
        "    text_tokenizer = Tokenizer()\n",
        "    text_tokenizer.fit_on_texts(sentences)\n",
        "    return text_tokenizer.texts_to_sequences(sentences), text_tokenizer\n"
      ],
      "metadata": {
        "id": "cGe1WXlWIKra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_sentences = []\n",
        "arabic_sentences = []\n",
        "\n",
        "for pair in pairs:\n",
        "    english_sentence = clean_sentence(pair[0])\n",
        "    arabic_sentence = clean_sentence(pair[1])\n",
        "\n",
        "\n",
        "    english_sentences.append(english_sentence)\n",
        "    arabic_sentences.append(arabic_sentence)\n",
        "\n",
        "#here column 0 and column 1\n"
      ],
      "metadata": {
        "id": "0O1KnZmbIMoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_text_tokenized, eng_text_tokenizer = tokenize(english_sentences)\n",
        "ara_text_tokenized, ara_text_tokenizer = tokenize(arabic_sentences)\n"
      ],
      "metadata": {
        "id": "TYC6DG5PIUJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_vocab = len(eng_text_tokenizer.word_index) + 1 # By adding 1 to the vocabulary length, we ensure that we reserve index 0 for this special token.\n",
        "arabic_vocab = len(ara_text_tokenizer.word_index) + 1\n",
        "\n"
      ],
      "metadata": {
        "id": "bPptL7jSIj-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_vocab\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7INBTHFeRVtu",
        "outputId": "fcd9ffba-067e-458d-fe87-2021999c04f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4086"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arabic_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY0L3wAdRXe6",
        "outputId": "58b99322-28a4-434d-ec7a-0df81fd3c539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11891"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_english_len = int(len(max(eng_text_tokenized, key=len)))\n",
        "max_arabic_len = int(len(max(ara_text_tokenized, key=len)))\n",
        "#For padding\n"
      ],
      "metadata": {
        "id": "OAHTBs2wIvdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_pad_sentence = pad_sequences(eng_text_tokenized, max_english_len, padding=\"post\")\n",
        "ara_pad_sentence = pad_sequences(ara_text_tokenized, max_arabic_len, padding=\"post\")\n"
      ],
      "metadata": {
        "id": "uQtMFCZ4Jocw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_pad_sentence = eng_pad_sentence.reshape(*eng_pad_sentence.shape, 1)\n",
        "ara_pad_sentence = ara_pad_sentence.reshape(*ara_pad_sentence.shape, 1)\n",
        "# reshape the padded English and Arabic sentences to add a third dimension of size 1\n",
        "\n"
      ],
      "metadata": {
        "id": "svPJfAqcKSYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequence = Input(shape=(max_english_len,), dtype='int32')\n",
        "embedding = Embedding(input_dim=english_vocab, output_dim=128)(input_sequence)\n",
        "#define the input layer (input_sequence) and apply an embedding layer (Embedding) to the input."
      ],
      "metadata": {
        "id": "-tdwedZkLI-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "^^^^^^^\n",
        "output_dim=128: The output_dim parameter sets the dimensionality of the dense embedding vectors. In this case, the embedding layer will generate dense vectors of size 128 for each word in the input sequence. This means each word in the vocabulary will be represented by a vector of length 128.\n",
        " 128 (or similar values) are often chose"
      ],
      "metadata": {
        "id": "rbH_LO-9MON0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Kt3Vvfkt4-AP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**bert modet**\n"
      ],
      "metadata": {
        "id": "J6B-deaz5AMh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XWDNtDSp5F6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "transformer_model = TFAutoModel.from_pretrained(\"bert-base-multilingual-cased\")"
      ],
      "metadata": {
        "id": "dj2cj0i7hklk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd231e19-f1f8-4441-a15f-86e8bb0d9cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_outputs = transformer_model(input_sequence)[0]\n",
        "decoder_inputs = Input(shape=(max_arabic_len,), dtype='int32')"
      ],
      "metadata": {
        "id": "5myajJVzh0Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding layer for the decoder inputs\n",
        "decoder_embedding = Embedding(input_dim=arabic_vocab, output_dim=128)(decoder_inputs)\n",
        "\n",
        "encoder_lstm = LSTM(64, return_sequences=True, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(embedding)\n",
        "\n",
        "decoder_lstm = LSTM(64, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])"
      ],
      "metadata": {
        "id": "hSdaYIhpiBXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#here is the Dense (fully connected) layer for the decoder output\n",
        "decoder_dense = Dense(arabic_vocab, activation='softmax')\n",
        "output = decoder_dense(decoder_outputs)\n"
      ],
      "metadata": {
        "id": "pRYMFOqViCiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "enc_dec_model = Model([input_sequence, decoder_inputs], output)\n",
        "\n",
        "# optimizer Adam with a learning rate of 0.001\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "\n",
        "enc_dec_model.compile(optimizer=optimizer, loss=sparse_categorical_crossentropy, metrics=['accuracy'])\n",
        "enc_dec_model.summary()\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "num_batches = len(eng_pad_sentence) // batch_size\n",
        "\n",
        "# Define the validation set\n",
        "val_size = 1000\n",
        "eng_pad_val = eng_pad_sentence[-val_size:]\n",
        "ara_pad_val = ara_pad_sentence[-val_size:]"
      ],
      "metadata": {
        "id": "4mUKEooKiIam",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9f00fcf-87e3-42a2-a399-045375d37cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 20)]         0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 20, 128)      523008      ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 17, 128)      1522048     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 20, 64),     49408       ['embedding[0][0]']              \n",
            "                                 (None, 64),                                                      \n",
            "                                 (None, 64)]                                                      \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, 17, 64),     49408       ['embedding_1[0][0]',            \n",
            "                                 (None, 64),                      'lstm[0][1]',                   \n",
            "                                 (None, 64)]                      'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 17, 11891)    772915      ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,916,787\n",
            "Trainable params: 2,916,787\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 15\n",
        "total_loss = 0\n",
        "total_accuracy = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in range(num_batches):\n",
        "        start_index = batch * batch_size\n",
        "        end_index = (batch + 1) * batch_size\n",
        "        eng_batch = eng_pad_sentence[start_index:end_index]\n",
        "        ara_batch = ara_pad_sentence[start_index:end_index]\n",
        "\n",
        "        loss, accuracy = enc_dec_model.train_on_batch([eng_batch, ara_batch], ara_batch)\n",
        "\n",
        "        total_loss += loss\n",
        "        total_accuracy += accuracy\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_accuracy = total_accuracy / num_batches\n",
        "\n",
        "    # Validation loss and accuracy\n",
        "    val_loss, val_accuracy = enc_dec_model.evaluate([eng_pad_val, ara_pad_val], ara_pad_val, verbose=0)\n",
        "\n",
        "    print(\"Epoch: {}/{} - Avg. Loss: {:.4f} - Avg. Accuracy: {:.4f} - Val Loss: {:.4f} - Val Accuracy: {:.4f}\".format(\n",
        "        epoch + 1, num_epochs, avg_loss, avg_accuracy, val_loss, val_accuracy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzguALYQ97vv",
        "outputId": "9bd1788e-fd53-4a81-e91f-73fa1b571b7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/15 - Avg. Loss: 3.3035 - Avg. Accuracy: 0.7275 - Val Loss: 3.6027 - Val Accuracy: 0.5478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert logits to a sentence\n",
        "def logits_to_sentence(logits, tokenizer):\n",
        "    index_to_words = {idx: word for word, idx in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = ''\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ],
      "metadata": {
        "id": "T61bFaXZ7j2s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}