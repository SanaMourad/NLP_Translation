{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding, Dot, Concatenate, Bidirectional, Permute\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy\n"
      ],
      "metadata": {
        "id": "H-gloscbpIiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_data = '/content/ara_eng.txt'\n",
        "translation_file = open(path_to_data, \"r\", encoding='utf-8')\n",
        "raw_data = translation_file.read()\n",
        "translation_file.close()\n"
      ],
      "metadata": {
        "id": "23iIodq5pLBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = raw_data.split('\\n')\n",
        "pairs = [sentence.split('\\t') for sentence in raw_data]\n",
        "pairs = pairs[1000:11000]\n"
      ],
      "metadata": {
        "id": "2A7hkyaBpN-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def clean_sentence(sentence):\n",
        "    lower_case_sent = sentence.lower()\n",
        "    string_punctuation = string.punctuation + \"¡\" + '¿'\n",
        "\n",
        "    for punctuation in string_punctuation:\n",
        "        lower_case_sent = lower_case_sent.replace(punctuation, '')\n",
        "\n",
        "    return lower_case_sent"
      ],
      "metadata": {
        "id": "xCcZxFQCpQnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentences):\n",
        "    text_tokenizer = Tokenizer()\n",
        "    text_tokenizer.fit_on_texts(sentences)\n",
        "    return text_tokenizer.texts_to_sequences(sentences), text_tokenizer\n"
      ],
      "metadata": {
        "id": "mgD-HxDTpTwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_sentences = []\n",
        "arabic_sentences = []\n",
        "\n",
        "for pair in pairs:\n",
        "    english_sentence = clean_sentence(pair[0])\n",
        "    arabic_sentence = clean_sentence(pair[1])\n",
        "\n",
        "    english_sentences.append(english_sentence)\n",
        "    arabic_sentences.append(arabic_sentence)\n",
        "\n",
        "#here column 0 and column 1\n"
      ],
      "metadata": {
        "id": "aYU0Dr73pYGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_text_tokenized, eng_text_tokenizer = tokenize(english_sentences)\n",
        "ara_text_tokenized, ara_text_tokenizer = tokenize(arabic_sentences)\n"
      ],
      "metadata": {
        "id": "v40uQpOJpcxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_vocab = len(eng_text_tokenizer.word_index) + 1 # By adding 1 to the vocabulary length, we ensure that we reserve index 0 for this special token.\n",
        "arabic_vocab = len(ara_text_tokenizer.word_index) + 1\n"
      ],
      "metadata": {
        "id": "PoevchZJpdaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYjciqPhpf1L",
        "outputId": "672b993d-3703-46d0-bded-258cf572a6f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4086"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arabic_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJwhr07dpiAV",
        "outputId": "fc9def86-3c37-42a2-e41a-389eb85d6334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11891"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_english_len = int(len(max(eng_text_tokenized, key=len)))\n",
        "max_arabic_len = int(len(max(ara_text_tokenized, key=len)))\n",
        "#For padding\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7mKDlrvApo8Y",
        "outputId": "a9ef9390-8176-431f-f482-6839c0750625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n These lines calculate the maximum length of the tokenized English and Arabic sentences and convert them to integers.\\nThese maximum lengths will be later used for padding the tokenized sequences to a fixed length, ensuring all sentences have the same shape and can be processed by the model efficiently.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_pad_sentence = pad_sequences(eng_text_tokenized, max_english_len, padding=\"post\")\n",
        "ara_pad_sentence = pad_sequences(ara_text_tokenized, max_arabic_len, padding=\"post\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "bUtqlmZWpt3w",
        "outputId": "ddbf7e35-512b-452e-c337-2890f4002b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n padding=\"post\": This parameter specifies the padding position. It can take one of two values: \"pre\" or \"post\". Here, \"post\" is specified, indicating that padding will\\n  be added at the end (or right) of each sequence.\\n Use \"post\" padding when:\\n\\nThe order of the elements in the sequence is important, and the padding should be added at the end of the sequence.\\nThe end of the sequence carries more significance or represents a meaningful boundary.\\nFor tasks like language modeling, where the model needs to predict the next word in the sequence, it is common to pad the sequences at the end.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_pad_sentence = eng_pad_sentence.reshape(*eng_pad_sentence.shape, 1)\n",
        "ara_pad_sentence = ara_pad_sentence.reshape(*ara_pad_sentence.shape, 1)\n",
        "# reshape the padded English and Arabic sentences to add a third dimension of size 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "9V7jXGgFpv05",
        "outputId": "97de0ea4-71b7-4700-d951-53cce4a130ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nUsing the * operator in this context simplifies the code and makes it more readable by avoiding the need to manually extract the dimensions from the shape tuple.\\n\\nIn summary, the * operator is used to unpack the elements of an iterable and pass them as separate arguments to a function or method, providing a convenient way to work with tuples or lists of values.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequence = Input(shape=(max_english_len,), dtype='int32')\n",
        "embedding = Embedding(input_dim=english_vocab, output_dim=128)(input_sequence)\n",
        "#define the input layer (input_sequence) and apply an embedding layer (Embedding) to the input."
      ],
      "metadata": {
        "id": "8_pkRyl1py3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XSOnEa6XZE0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ffe4f8-e03a-468e-85e8-049e3ac82c97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 20)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 20, 256)      1046016     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  [(None, 20, 512),    1050624     ['embedding_1[0][0]']            \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 17, 256)      3044096     ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 512)          0           ['bidirectional[0][1]',          \n",
            "                                                                  'bidirectional[0][3]']          \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 512)          0           ['bidirectional[0][2]',          \n",
            "                                                                  'bidirectional[0][4]']          \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, 17, 512),    1574912     ['embedding_2[0][0]',            \n",
            "                                 (None, 512),                     'concatenate[0][0]',            \n",
            "                                 (None, 512)]                     'concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " dot (Dot)                      (None, 17, 20)       0           ['lstm_1[0][0]',                 \n",
            "                                                                  'bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " attention (Activation)         (None, 17, 20)       0           ['dot[0][0]']                    \n",
            "                                                                                                  \n",
            " dot_1 (Dot)                    (None, 17, 512)      0           ['attention[0][0]',              \n",
            "                                                                  'bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 17, 1024)     0           ['dot_1[0][0]',                  \n",
            "                                                                  'lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 17, 11891)    12188275    ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 18,903,923\n",
            "Trainable params: 18,903,923\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch: 1/10 - Avg. Loss: 1.9125 - Avg. Accuracy: 0.7995 - Val Loss: 1.5245 - Val Accuracy: 0.8076\n",
            "Epoch: 2/10 - Avg. Loss: 0.7829 - Avg. Accuracy: 0.8979 - Val Loss: 0.3978 - Val Accuracy: 0.9454\n",
            "Epoch: 3/10 - Avg. Loss: 0.3186 - Avg. Accuracy: 0.9453 - Val Loss: 0.1413 - Val Accuracy: 0.9789\n",
            "Epoch: 4/10 - Avg. Loss: 0.0862 - Avg. Accuracy: 0.9805 - Val Loss: 0.1384 - Val Accuracy: 0.9858\n",
            "Epoch: 5/10 - Avg. Loss: 0.0309 - Avg. Accuracy: 0.9945 - Val Loss: 0.0956 - Val Accuracy: 0.9901\n",
            "Epoch: 6/10 - Avg. Loss: 0.0194 - Avg. Accuracy: 0.9972 - Val Loss: 0.0654 - Val Accuracy: 0.9940\n",
            "Epoch: 7/10 - Avg. Loss: 0.0076 - Avg. Accuracy: 0.9988 - Val Loss: 0.0588 - Val Accuracy: 0.9955\n",
            "Epoch: 8/10 - Avg. Loss: 0.0020 - Avg. Accuracy: 0.9997 - Val Loss: 0.0569 - Val Accuracy: 0.9956\n",
            "Epoch: 9/10 - Avg. Loss: 0.0008 - Avg. Accuracy: 0.9999 - Val Loss: 0.0562 - Val Accuracy: 0.9956\n",
            "Epoch: 10/10 - Avg. Loss: 0.0004 - Avg. Accuracy: 1.0000 - Val Loss: 0.0575 - Val Accuracy: 0.9956\n",
            "The English sentence is: well lets go\n",
            "The Arabic sentence is: إذاً هيا بنا نذهب\n",
            "The predicted Arabic sentence is:\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "إذاً هيا بنا نذهب             \n"
          ]
        }
      ],
      "source": [
        "# Encoder\n",
        "input_sequence = Input(shape=(max_english_len,))\n",
        "embedding = Embedding(input_dim=english_vocab, output_dim=256)(input_sequence)\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(256, return_sequences=True, return_state=True))(embedding)\n",
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_arabic_len,))\n",
        "decoder_embedding = Embedding(input_dim=arabic_vocab, output_dim=256)(decoder_inputs)\n",
        "decoder_lstm = LSTM(512, return_sequences=True, return_state=True)  # Update the LSTM size to 512\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
        "\n",
        "# Attention\n",
        "attention = Dot(axes=[2, 2])([decoder_outputs, encoder_outputs])\n",
        "attention = Activation('softmax', name='attention')(attention)\n",
        "context = Dot(axes=[2, 1])([attention, encoder_outputs])\n",
        "\n",
        "decoder_combined_context = Concatenate()([context, decoder_outputs])\n",
        "\n",
        "# Output layer\n",
        "decoder_dense = Dense(arabic_vocab, activation='softmax')\n",
        "output = decoder_dense(decoder_combined_context)\n",
        "\n",
        "# model\n",
        "enc_dec_model = Model([input_sequence, decoder_inputs], output)\n",
        "\n",
        "learning_rate = 0.01\n",
        "optimizer = Adam(learning_rate)\n",
        "\n",
        "enc_dec_model.compile(loss=sparse_categorical_crossentropy,\n",
        "                      optimizer=optimizer,\n",
        "                      metrics=['accuracy'])\n",
        "enc_dec_model.summary()\n",
        "\n",
        "batch_size = 32\n",
        "num_batches = len(eng_pad_sentence) // batch_size\n",
        "\n",
        "#valadation\n",
        "val_size = 1000\n",
        "eng_pad_val = eng_pad_sentence[-val_size:]\n",
        "ara_pad_val = ara_pad_sentence[-val_size:]\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "\n",
        "    for batch in range(num_batches):\n",
        "        # Extract a mini-batch\n",
        "        start_index = batch * batch_size\n",
        "        end_index = (batch + 1) * batch_size\n",
        "        eng_batch = eng_pad_sentence[start_index:end_index]\n",
        "        ara_batch = ara_pad_sentence[start_index:end_index]\n",
        "\n",
        "        # Update the model's parameter s for better run\n",
        "        loss, accuracy = enc_dec_model.train_on_batch([eng_batch, ara_batch], ara_batch)\n",
        "\n",
        "        total_loss += loss\n",
        "        total_accuracy += accuracy\n",
        "\n",
        "\n",
        "    val_loss, val_accuracy = enc_dec_model.evaluate([eng_pad_val, ara_pad_val], ara_pad_val, verbose=0)\n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_accuracy = total_accuracy / num_batches\n",
        "\n",
        "    print(\"Epoch: {}/{} - Avg. Loss: {:.4f} - Avg. Accuracy: {:.4f} - Val Loss: {:.4f} - Val Accuracy: {:.4f}\".format(\n",
        "        epoch + 1, num_epochs, avg_loss, avg_accuracy, val_loss, val_accuracy))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def logits_to_sentence(logits, tokenizer):\n",
        "    index_to_words = {idx: word for word, idx in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = ''\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
        "\n",
        "# Example sentence translation\n",
        "index = 10\n",
        "print(\"The English sentence is: {}\".format(english_sentences[index]))\n",
        "print(\"The Arabic sentence is: {}\".format(arabic_sentences[index]))\n",
        "print('The predicted Arabic sentence is:')\n",
        "predicted_sentence = logits_to_sentence(\n",
        "    enc_dec_model.predict([eng_pad_sentence[index:index + 1], ara_pad_sentence[index:index + 1]])[0],\n",
        "    ara_text_tokenizer)\n",
        "print(predicted_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfkHlTZc-30Z",
        "outputId": "a19c73f5-40f2-4e5b-faeb-615e7d0733d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The English sentence is: were obedient\n",
            "The Arabic sentence is: نحن مطيعون\n",
            "The predicted Arabic sentence is:\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "نحن مطيعون               \n"
          ]
        }
      ]
    }
  ]
}